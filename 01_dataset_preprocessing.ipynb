{"cells":[{"cell_type":"markdown","metadata":{"executionInfo":{"elapsed":85,"status":"ok","timestamp":1769023118111,"user":{"displayName":"Martin Blouin","userId":"06053736196572450606"},"user_tz":300},"id":"ND9e5No84eal","outputId":"88aa9296-3540-4bc2-c0d7-a9e39e5f17b0"},"source":["# Cours #1 - Introduction au Machine Learning pour la Classification des Milieux Humides du Lac Ontario\n","\n","[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/geo-stack/cours_ml_envirocan/blob/main/01_dataset_preprocessing.ipynb)"]},{"cell_type":"markdown","metadata":{"id":"DmOSbOQrB-7x"},"source":["## 1. Configuration de l'environnement"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"omKXhUeaB-7y"},"outputs":[],"source":["print(\"Installation de 'gdown' pour le téléchargement du dataset...\", end='')\n","!pip install --upgrade gdown -q\n","print(' OK')\n","\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import matplotlib.ticker as ticker\n","from matplotlib.transforms import ScaledTranslation\n","import os\n","import gdown\n","\n","from sklearn.metrics import accuracy_score\n","from sklearn.model_selection import train_test_split\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.preprocessing import StandardScaler\n","\n","print('Environnement configuré avec succès!')"]},{"cell_type":"markdown","metadata":{"id":"Auoju13oB-70"},"source":["## 2. Téléchargement et chargement du jeu de données"]},{"cell_type":"markdown","source":["### 2.1. Téléchargement du jeu de données\n","\n","> ⚠️ _À noter que l’accès au téléchargement du fichier de données est limité à la période du cours. Passé cette période, le fichier ne sera plus accessible._"],"metadata":{"id":"u1_2fyR3T2Jn"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"cXbWPNORB-70"},"outputs":[],"source":["INPUT_FILENAME = 'CWRM_GLFEI_vegetation_dataset_v2_extracted_landscape.csv'\n","\n","print(\"Téléchargement du jeu de données en cours...\")\n","\n","gdown.download(\n","    f'https://drive.google.com/uc?id=11oKUaykdZ0G5hpns6nct_lkVvIzo_3w3',\n","    INPUT_FILENAME,\n","    quiet=True\n","    )\n","\n","print(\"Téléchargement terminé.\")"]},{"cell_type":"markdown","source":["### 2.2. Chargement du jeu de données"],"metadata":{"id":"zTpk6QitT-3q"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"0ikEC9YA34BD"},"outputs":[],"source":["dtype_spec = {\n","    'Land_mask': 'boolean',\n","    'DOMINANCE': 'string',\n","    }\n","\n","df = pd.read_csv(INPUT_FILENAME, delimiter=\";\", dtype=dtype_spec)\n","df.head()"]},{"cell_type":"markdown","source":["## 3. Exploration du jeu de données et sélection des variables\n","\n","Le jeu de données contient des observations de milieux humides du Lac Ontario qui sont directement connectés et influencés par le Lac. Les données proviennent de différentes sources, pour lesquels des variables physiques représentant l'hydro-période, hydrologie, la topographie et le paysage ont été calculées (profondeur, % inondé, période de fluctuation, type de sol, type hydrogéomorphique, pente, etc.). Cette base de données est utilisée pour calibrer un classificateur prédisant une classe de milieu humide à un point donné qui est intégré dans le Coastal Wetland Response Model (CWRM).\n","\n","<details>\n","<summary>Tableau 1 : Description des classes de milieu humide à prédire</summary>\n","\n","<br>\n","\n","| Acronyme | Description                          |\n","|----------|--------------------------------------|\n","| OW       | Open Water                          |\n","| SAV      | Submerged Aquatic Vegetation        |\n","| EM       | Emergent marshes                    |\n","| WM       | Wet Meadow                          |\n","| SW       | Swamp                               |\n","| UPL      | Upland                              |\n","\n","</details>\n","\n","<br>\n","\n","<details>\n","<summary>Tableau 2 : Nom et description des colonnes du jeu de données</summary>\n","  \n","<br>\n","\n","| N° | Colonne                 | Description                                                                                 |\n","|----|-------------------------|---------------------------------------------------------------------------------------------|\n","|  1 | SAMPLE_ID               | Identifiant unique de l'échantillon                                                        |\n","|  2 | PT_ID                   | Identifiant unique du point de grille                                                      |\n","|  3 | SITE_ID                 | Identiifiant unique du site de milieux humide                                              |\n","|  4 | TILE_ID                 | Identifiant unique de la tuile du DEM                                                      |\n","|  5 | LON                     | Coordonnée Longitude (degrés décimaux)                                                     |\n","|  6 | LAT                     | Coordonnée Latitude (degrés décimaux)                                                      |\n","|  7 | XVAL                    | Coordonnée Y en UTM (m) (EPSG :32617 ou 32618)                                             |\n","|  8 | YVAL                    | Coordonnée Y en UTM (m) (EPSG :32617 ou 32618)                                             |\n","|  9 | ZVAL_1m                 | Coordonnée Z à partir du DEM 1m en IGLD (m) pour quadrats seulement                        |\n","| 10 | ZVAL                    | Coordonnée Z à partir du DEM 10m en IGLD (m)                                               |\n","| 11 | YEAR                    | Année d’échantillonnage                                                                    |\n","| 12 | SOURCE                  | Jeu de données source du relevé                                                            |\n","| 13 | SOURCE_TYPE             | 'Field Survey' ou 'Photo Interpretation'                                                   |\n","| 14 | SAV                     | % recouvrement total des espèces de SAV (quadrats seulement)                               |\n","| 15 | EM                      | % recouvrement total des espèces de EM (quadrats seulement)                                |\n","| 16 | WM                      | % recouvrement total des espèces de WM (quadrats seulement)                                |\n","| 17 | SW                      | % recouvrement total des espèces de SW (quadrats seulement)                                |\n","| 18 | UPL                     | % recouvrement total des espèces de UPL (quadrats seulement)                               |\n","| 19 | DOMINANCE               | Permet de savoir si le quadrat était mixé ou dominant lors de l’assignation de la classe   |\n","| 20 | CLASSIF                 | Classe assignée                                                                            |\n","| 21 | UTM                     | Zone UTM (17 ou 18)                                                                        |\n","| 22 | SLOPE_10m               | Pente sur résolution spatiale de 10m                                                       |\n","| 23 | CURVATURE_10m           | Pente seconde sur résolution spatiale de 10m                                               |\n","| 24 | SLOPE_30m_resampled     | Pente sur résolution spatiale de 30m                                                       |\n","| 25 | CURVATURE_30m_resampled | Pente seconde sur résolution spatiale de 30m                                               |\n","| 26 | SLOPE_30m_resampled     | Pente sur résolution spatiale de 30m                                                       |\n","| 27 | CURVATURE_30m_resampled | Pente seconde sur résolution spatiale de 30m                                               |\n","| 28 | SLOPE_130m_resampled    | Pente sur résolution spatiale de 130m                                                      |\n","| 29 | CURVATURE_130m_resampled| Pente seconde sur résolution spatiale de 130m                                              |\n","| 30 | SLOPE_250m_resampled    | Pente sur résolution spatiale de 250m                                                      |\n","| 31 | CURVATURE_250m_resampled| Pente seconde sur résolution spatiale de 250m                                              |\n","| 32 | SLOPE_310m_resampled    | Pente sur résolution spatiale de 310m                                                      |\n","| 33 | CURVATURE_310m_resampled| Pente seconde sur résolution spatiale de 310m                                              |\n","| 34 | hMin_SC1                | Profondeur minimale; m.   (année précédente)                                               |\n","| 35 | hMoy_SC1                | Profondeur moyenne; m.   (année précédente)                                                |\n","| 36 | hMax_SC1                | Profondeur maximale; m.   (année précédente)                                               |\n","| 37 | Flood_SC1               | % temps inondé (année précédente)                                                          |\n","| 38 | PcFMoy_SC1              | % temps strictement inondé; sans fluctuations (année précédente)                          |\n","| 39 | pcDMoy_SC1              | % temps strictement exondé; sans fluctuations (année précédente)                          |\n","| 40 | pcFluxMoy_SC1           | % temps fluctuations (année précédente)                                                    |\n","| 41 | saPxMax_SC1             | Puissance maximale normalisée de l'analyse en ondelettes (année précédente)                |\n","| 42 | saPMoy_SC1              | Puissance moyenne normalisée de l'analyse en ondelettes (année précédente)                 |\n","| 43 | TcMin_SC1               | Période minimale des fluctuations (année précédente)                                       |\n","| 44 | TcMax_SC1               | Période maximale des fluctuations (année précédente)                                       |\n","| 45 | TcMoy_SC1               | Période moyenne des fluctuations (année précédente)                                        |\n","| 46 | xTcMax_SC1              | Période maximale détectable des fluctuations (année précédente)                            |\n","| 47 | hMin_PSC2               | Profondeur minimale; m. (2 ans précédents)                                                |\n","| 48 | hMoy_PSC2               | Profondeur moyenne; m.  (2 ans précédents)                                                |\n","| 49 | hMax_PSC2               | Profondeur maximale; m.  (2 ans précédents)                                               |\n","| 50 | Flood_PSC2              | % temps inondé (2 ans précédents)                                                         |\n","| 51 | PcFMoy_PSC2             | % temps strictement inondé; sans fluctuations (2 ans précédents)                          |\n","| 52 | pcDMoy_PSC2             | % temps strictement exondé; sans fluctuations (2 ans précédents)                          |\n","| 53 | pcFluxMoy_ PSC2         | % temps fluctuations (2 ans précédents)                                                   |\n","| 54 | saPxMax_ PSC2           | Puissance maximale normalisée de l'analyse en ondelettes (2 ans précédents)               |\n","| 55 | saPMoy_ PSC2            | Puissance moyenne normalisée de l'analyse en ondelettes (2 ans précédents)                |\n","| 56 | TcMin_ PSC2             | Période minimale des fluctuations (2 ans précédents)                                      |\n","| 57 | TcMax_ PSC2             | Période maximale des fluctuations (2 ans précédents)                                      |\n","| 58 | TcMoy_ PSC2             | Période moyenne des fluctuations (2 ans précédents)                                       |\n","| 59 | xTcMax_ PSC2            | Période maximale détectable des fluctuations (2 ans précédents)                           |\n","| 60 | hMin_PSC3               | Profondeur minimale; m. (3 ans précédents)                                                |\n","| 61 | hMoy_PSC3               | Profondeur moyenne; m.  (3 ans précédents)                                                |\n","| 62 | hMax_PSC3               | Profondeur maximale; m.  (3 ans précédents)                                               |\n","| 63 | Flood_PSC3              | % temps inondé (3 ans précédents)                                                         |\n","| 64 | PcFMoy_PSC3             | % temps strictement inondé; sans fluctuations (3 ans précédents)                          |\n","| 65 | pcDMoy_PSC3             | % temps strictement exondé; sans fluctuations (3 ans précédents)                          |\n","| 66 | pcFluxMoy_ PSC3         | % temps fluctuations (3 ans précédents)                                                   |\n","| 67 | saPxMax_ PSC3           | Puissance maximale normalisée de l'analyse en ondelettes (3 ans précédents)               |\n","| 68 | saPMoy_ PSC3            | Puissance moyenne normalisée de l'analyse en ondelettes (3 ans précédents)                |\n","| 69 | TcMin_ PSC3             | Période minimale des fluctuations (3 ans précédents)                                      |\n","| 70 | TcMax_ PSC3             | Période maximale des fluctuations (3 ans précédents)                                      |\n","| 71 | TcMoy_ PSC3             | Période moyenne des fluctuations (3 ans précédents)                                       |\n","| 72 | xTcMax_ PSC3            | Période maximale détectable des fluctuations (3 ans précédents)                           |\n","| 73 | Organic                 | Type de sol organique (binaire)                                                            |\n","| 74 | Mineral                 | Type de sol minéral (binaire)                                                              |\n","| 75 | Rock                    | Type de sol rocheux (binaire)                                                              |\n","| 76 | Calcareous              | Type de sol calcaire (binaire)                                                             |\n","| 77 | Non-Calcar              | Type de sol non-calcaire (binaire)                                                         |\n","| 78 | Coarse                  | Type de substrat grossier (binaire)                                                        |\n","| 79 | Fine                    | Type de substrat fin (binaire)                                                             |\n","| 80 | Class_Land              | Classe d'occupation du sol                                                                 |\n","| 81 | Land_mask               | Si masqué ou non selon la classe d'occupation du sol (boolean)                             |\n","| 82 | HGM_BL                  | Type hydrogéomorphique : Barrier Lagoon (binaire)                                          |\n","| 83 | HGM_LOE                 | Type hydrogéomorphique : Baie Ouverte Lacustre (binaire)                                   |\n","| 84 | HGM_LPP                 | Type hydrogéomorphique : Baie Protégée Lacustre (binaire)                                  |\n","| 85 | HGM_RRB                 | Type hydrogéomorphique : Embouchure de rivière Barrée (binaire)                            |\n","| 86 | HGM_RRO                 | Type hydrogéomorphique : Embouchure de rivière Ouverte (binaire)                           |\n","\n","</details>\n","<br>"],"metadata":{"id":"gESTaZrWUKmF"}},{"cell_type":"markdown","source":["### 3.1. Distribution de la variable cible\n","\n","L'histogramme révèle un **fort déséquilibre** entre les classes : la classe **UPL** (milieux terrestres) représente 45% du dataset (491k observations), tandis que **OW**, **WM** et **SW** ne comptent que 6% chacune (≈66k observations). Ce déséquilibre nécessite une attention particulière : sans précaution, un modèle pourrait privilégier les classes majoritaires, et une exactitude globale élevée pourrait cacher de mauvaises performances sur les classes minoritaires. Dans les sections suivantes, nous utiliserons systématiquement une **stratification** (`stratify=y`) lors du partitionnement des données pour garantir que chaque ensemble conserve cette distribution.\n"],"metadata":{"id":"tUMF3WM7U8lJ"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZnyMbAts6uTn"},"outputs":[],"source":["# Génération d'un histogramme du nombre d'observations par classe ('CLASSIF').\n","\n","counts = df.CLASSIF.value_counts()\n","\n","fig, ax = plt.subplots()\n","bars = ax.bar(counts.index, counts.values, color='skyblue')\n","ax.set_xlabel('Classes', fontsize=14, labelpad=10)\n","ax.set_ylabel('Nombre', fontsize=14, labelpad=10)\n","ax.set_title(\"Histogramme des classes de milieu humide\")\n","ax.yaxis.grid(True, linestyle='--', alpha=0.7)\n","ax.set_axisbelow(True)\n","ax.yaxis.set_major_formatter(\n","    ticker.FuncFormatter(\n","        lambda x, _: f'{x/1000:0.0f}k' if x >= 1000 else int(x)\n","        ))\n","\n","# Ajout padding vertical (10 % au-dessus du max).\n","ax.set_ylim(top=counts.max() * 1.11)\n","\n","# Ajout de la valeur et proportion au dessus de chaque barre.\n","ntot = len(df)\n","for bar in bars:\n","    x = bar.get_x() + bar.get_width() / 2\n","    count = bar.get_height()\n","    perc = count / ntot * 100\n","    ax.text(x, count,\n","            f\"{count:,}\".replace(\",\", \" \") + \"\\n\" + f\"({perc:0.1f}%)\",\n","            ha='center', va='bottom', fontsize=10,\n","            transform=ax.transData + ScaledTranslation(\n","                0, 1/72, ax.figure.dpi_scale_trans)\n","            )\n","\n","fig.tight_layout()"]},{"cell_type":"markdown","source":["### 3.2. Sélection des variables explicatives\n","\n","Dans un premier temps, on retire du jeu de données les variables pouvant biaiser la prédiction (ex: localisation, identifiants).\n","\n","Ensuite, les modèles sont généralement calibrés en utilisant les variables calculées sur une période donnée (1, 2 ou 3 ans précédents). Les regroupement d'années ne sont pas mélangés. On choisi donc ici une période (1, 2 ou 3 ans) et on ne garde que les variables correspondantes.\n"],"metadata":{"id":"FMAoZRm8lOIx"}},{"cell_type":"code","source":["# On retire les variables pouvant biaiser la prédiction.\n","cols_to_drop = [\n","    'SAMPLE_ID', 'PT_ID', 'SITE_ID', 'TILE_ID', 'LON', 'LAT', 'XVAL',\n","    'YVAL', 'ZVAL', 'YEAR', 'SOURCE', 'SOURCE_TYPE', 'UTM'\n","    ]\n","\n","df_features = df.drop(columns=cols_to_drop)\n","\n","print(\n","    f\"Les {len(cols_to_drop)} colonnes suivantes ont été retirées du jeu \"\n","    f\"de données car elles pourraient potentiellement biaiser le \"\n","    f\"modèle de prédiction :\\n\"\n","    + \"\\n\".join(f\"   - {col}\" for col in cols_to_drop)\n","    )\n","\n","n_from = df.shape[1]\n","n_to = df_features.shape[1]\n","print(f\"\\nNombre de colonnes : {n_from} → {n_to}\")\n","\n","# On ne garde que les variables calculées sur la période sélectionnée.\n","\n","selected_period = 1\n","\n","varnames = ['hMin', 'hMoy', 'hMax', 'Flood', 'pcFMoy', 'pcDMoy', 'pcFluxMoy',\n","            'saPxMax', 'saPMoy', 'TcMin', 'TcMax', 'TcMoy', 'xTcMax', 'xTqMax']\n","\n","cols_to_drop = []\n","for period in [1, 2, 3]:\n","    if period == selected_period:\n","        continue\n","\n","    for varname in varnames:\n","        colname = (\n","            varname + (f'_SC{period}' if period == 1 else f'_PSC{period}')\n","            )\n","        cols_to_drop.append(colname)\n","\n","df_features = df_features.drop(columns=cols_to_drop)\n","\n","print(\n","    f\"\\nLes {len(cols_to_drop)} colonnes suivantes ont été retirées du jeu \"\n","    f\"de données car elles ne correspondent pas à la période de calcul \"\n","    f\"sélectionnée :\\n\"\n","    + \"\\n\".join(f\"   - {col}\" for col in cols_to_drop) + \"\\n\"\n","    )\n","\n","n_from = n_to\n","n_to = df_features.shape[1]\n","print(f\"\\nNombre de colonnes : {n_from} → {n_to}\")\n"],"metadata":{"id":"jz0dvbKYlSXU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 4. Gestion des données manquantes"],"metadata":{"id":"vkoKyMadUu7e"}},{"cell_type":"markdown","source":["### 4.1. Quantification et visualisation des données manquantes"],"metadata":{"id":"MBDjnt6XgBI7"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"fGuY4zEs643W"},"outputs":[],"source":["# Quantification des données manquantes\n","nan_perc = df_features.isna().mean()\n","\n","# N'inclure que les variables avec au moins une valeur manquante.\n","nan_perc_filt = nan_perc[nan_perc > 0].sort_values(ascending=False)\n","\n","# Afficher les résultats dans un graphique.\n","fig, ax = plt.subplots(figsize=(10, 6))\n","bars = ax.bar(\n","    nan_perc_filt.index,\n","    nan_perc_filt.values * 100,\n","    color='salmon'\n","    )\n","ax.set_title('Variables avec valeurs manquantes (NaN > 0)')\n","ax.set_ylabel('Proportion de NaNs [%]', fontsize=14, labelpad=10)\n","ax.yaxis.grid(True, linestyle='--', alpha=0.7)\n","ax.set_axisbelow(True)\n","ax.set_yticks(np.arange(0, 110, 10))\n","for label in ax.get_xticklabels():\n","    label.set_fontsize(8)\n","    label.set_rotation(45)\n","    label.set_ha('right')\n","\n","fig.tight_layout()"]},{"cell_type":"markdown","source":["### 4.2. Suppression des colonnes avec valeurs manquantes excessives\n","\n","On élimine les colonnes avec un % de données manquantes supérieure à 30%.\n","\n","On élimine également la variable **TcMin** du jeu de données car celle-ci ne contient que des zéros."],"metadata":{"id":"cUInjT4bcBeQ"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"BBxYkD6tF9qd"},"outputs":[],"source":["nan_perc = df_features.isna().mean()\n","cols_to_drop = nan_perc[nan_perc > 0.3].index.tolist()\n","\n","df_features_clean = df_features.drop(columns=cols_to_drop)\n","\n","print(\n","    f\"Les {len(cols_to_drop)} colonnes suivantes ont été retirées du jeu \"\n","    f\"de données car elles contiennent plus de 30% de valeurs manquantes :\\n\"\n","    + \"\\n\".join(f\"   - {col} ({nan_perc[col] * 100:0.1f}%)\" for\n","                col in cols_to_drop)\n","    )\n","\n","if selected_period == 1:\n","    cols_to_drop = ['TcMin_SC1']\n","else:\n","    cols_to_drop = [f'TcMin_PSC{selected_period}']\n","\n","df_features_clean = df_features_clean.drop(columns=cols_to_drop)\n","\n","print(\"\\nLa variable 'TcMin' a également été retirée, car elle ne \"\n","      \"contient que des zéro.\")\n","\n","print(f\"\\nNombre de colonnes : \"\n","      f\"{df_features.shape[1]} → {df_features_clean.shape[1]}\")"]},{"cell_type":"markdown","source":["### 4.3. Imputation des valeurs manquantes et encodage des variables catégorielles\n","\n","Pour traiter les valeurs manquantes restantes dans le jeu de données, on\n","distingue deux approches :\n","\n","1. Supprimer les lignes contenant des valeurs manquantes (perte d'information)\n","\n","2. Imputer les valeurs manquantes, c'est-à-dire les remplacer par une valeur estimée\n","\n","Dans la cellule ci-dessous, nous choisissons d'imputer les valeurs manquantes à l'aide de stratégies adaptées au type de variable. Nous utilisons pour cela la classe `ColumnTransformer`, qui permet d'appliquer sélectivement les bons traitements à chaque groupe de variables.\n","\n","On distingue deux types de variables :\n","\n","1. Pour les **variables numériques** : nous utilisons la méthode `SimpleImputer` de scikit-learn avec la stratégie de la moyenne, ce qui permet de remplacer chaque donnée manquante par la moyenne des valeurs observées dans la colonne concernée.\n","\n","2. Pour les **variables catégorielles** : nous utilisons l'encodeur ordinal\n"," (`OrdinalEncoder`) de scikit-learn. Cette transformation attribue un code numérique unique à chaque modalité rencontrée dans une colonne catégorielle.<br>Attention : cette méthode n'est généralement adéquate que si l'ordre attribué n'a pas d'importance, ou pour des algorithmes supervisés au sens large. Les éventuelles valeurs manquantes dans les colonnes catégorielles seront considérées comme une nouvelle catégorie inconnue et codées en conséquence."],"metadata":{"id":"B-JPhCWxbo3L"}},{"cell_type":"code","source":["from sklearn.compose import ColumnTransformer\n","from sklearn.preprocessing import OrdinalEncoder\n","from sklearn.impute import SimpleImputer\n","\n","# On définit les colonnes qui sont des variables catégorielles.\n","cat_vars = [\n","    'Calcareous', 'Class_Land', 'Coarse', 'Fine',\n","    'HGM_BL', 'HGM_LOE', 'HGM_LPP', 'HGM_RRB', 'HGM_RRO',\n","    'Land_mask', 'Mineral', 'Non-Calcar', 'Organic', 'Rock'\n","    ]\n","\n","# On définit les colonnes qui sont des variables numériques.\n","notnum = ((np.isin(df_features_clean.columns, cat_vars)) |\n","          (df_features_clean.columns == 'CLASSIF'))\n","num_vars = list(df_features_clean.columns[~notnum])\n","\n","# Création du ColumnTransformer, où l'on associe des transformateurs\n","# spécifiques à des colonnes spécifiques.\n","preprocessor = ColumnTransformer(\n","    transformers=[\n","        ('none', 'passthrough', ['CLASSIF']),\n","        ('num_imputer', SimpleImputer(strategy='mean'), num_vars),\n","        ('cat_imputer', OrdinalEncoder(), cat_vars),\n","        ],\n","    )\n","\n","# Application de la transformation pour imputer les valeurs manquantes.\n","df_features_nonan = pd.DataFrame(\n","    data=preprocessor.fit_transform(df_features_clean),\n","    columns=['CLASSIF'] + num_vars + cat_vars,\n","    index=df_features_clean.index\n","    )\n"],"metadata":{"id":"B2GCUbytpSzm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 5. Exploration des relations entre les variables explicatives"],"metadata":{"id":"Bro1sjLLlqFa"}},{"cell_type":"markdown","source":["### 5.1 Visualisation des relations entre variables\n","\n","L'exploration visuelle des relations entre variables permet d'identifier des patterns de séparation entre les classes et de comprendre quelles variables sont les plus discriminantes.\n","\n","Cela est réalisé à l'aide d'une **matrice de dispersion** (pairplot) qui montre les relations bivariées entre les variables explicatives sélectionnées, colorées par classe de milieu humide.\n","\n","Étant donné la taille du dataset (> 1 million d'observations), nous sous-échantillonnons stratifié à **1%** pour rendre la génération du graphique réalisable tout en préservant la distribution des classes."],"metadata":{"id":"Of2DEbEim_Yy"}},{"cell_type":"code","source":["import seaborn as sns\n","from sklearn.model_selection import train_test_split\n","\n","# Sélection des variables que l'on veut afficher.\n","cols = ['SLOPE_10m', 'CURVATURE_10m']\n","if selected_period == 1:\n","    cols += ['hMoy_SC1',\n","             'Flood_SC1',\n","             'pcFMoy_SC1',\n","             'pcDMoy_SC1',\n","             'TcMax_SC1']\n","else:\n","    cols += [f'hMoy_PSC{selected_period}',\n","             f'Flood_PSC{selected_period}',\n","             f'pcFMoy_PSC{selected_period}',\n","             f'pcDMoy_PSC{selected_period}',\n","             f'TcMax_PSC{selected_period}']\n","\n","# Sous-échantillonnage stratifié à 1% du dataset pour rendre\n","# le temps de production des graphiques raisonnable en raison du\n","# grand nombre d'échantillone (> 1 million) dans le jeu de données\n","# complet.\n","y_full = df_features_nonan['CLASSIF'].values\n","X_full = df_features_nonan[cols].values\n","X_sample, _, y_sample, _ = train_test_split(\n","    X_full, y_full,\n","    train_size=0.01,\n","    stratify=y_full\n","    )\n","\n","df_sample = pd.DataFrame(\n","    data=X_sample,\n","    columns=cols)\n","df_sample['CLASSIF'] = y_sample\n","\n","# Génération du graph 'pairplot'.\n","sns.set_theme(style=\"ticks\", font_scale=0.8)\n","ax = sns.pairplot(\n","    df_sample, hue=\"CLASSIF\",\n","    height=2, aspect=1.0,\n","    plot_kws={'s': 20},\n","    )"],"metadata":{"id":"TyFLgNDolfql"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 5.2 Matrice de corrélation des variables numériques\n","\n","La **matrice de corrélation** révèle les relations linéaires entre toutes les variables explicatives numériques. Cette analyse permet d'identifier la **multicolinéarité** (variables fortement corrélées entre elles) et les variables redondantes qui apportent peu d'information supplémentaire au modèle.\n","\n","Les valeurs de corrélation varient entre -1 (corrélation négative parfaite) et +1 (corrélation positive parfaite), avec 0 indiquant une absence de relation linéaire. Des corrélations élevées (|r| > 0.8) suggèrent que certaines variables mesurent des phénomènes similaires et pourraient être candidates à l'élimination pour simplifier le modèle."],"metadata":{"id":"jlf58bEem-UH"}},{"cell_type":"code","source":["corr_matrix = df_features_nonan[num_vars].corr()\n","\n","fig, ax = plt.subplots(figsize=(14, 10))\n","sns.set_theme(style=\"ticks\", font_scale=0.8)\n","sns.heatmap(\n","    corr_matrix, annot=True, cmap='coolwarm',\n","    ax=ax, cbar=False, annot_kws={\"fontsize\": 8})\n","fig.suptitle('Matrice de Corrélation')\n","fig.tight_layout()\n","plt.show()"],"metadata":{"id":"8HXdqLQ0m-ev"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zf2b5jQKdI4n"},"source":["## 6. Prédiction avec un modèle KNN (K Nearest neighbours)\n","\n","L'algorithme des k plus proches voisins (KNN) est une méthode de classification non paramétrique basée sur la proximité entre observations. Pour classifier un nouveau point, l'algorithme identifie les k observations les plus proches dans l'espace des variables (selon une distance euclidienne) et lui attribue la classe majoritaire parmi ces voisins. Cette approche est particulièrement adaptée aux problèmes où les classes présentent une structure spatiale claire dans l'espace des variables explicatives.\n","\n","\n"]},{"cell_type":"markdown","source":["### 6.1 Préparation des données pour le modèle KNN\n","\n","Avant d'entraîner le modèle KNN, nous devons effectuer quatre étapes de préparation :\n","\n","**1. Sélection des variables numériques**\n","\n","L'algorithme KNN repose sur le calcul de distances euclidiennes. Les variables catégorielles encodées numériquement introduisent un ordre artificiel qui fausse la mesure de similarité. Nous ne conservons donc que les variables numériques.\n","\n","**2. Définition des variables X (explicatives) et y (cible)**\n","\n","- Variable cible (y) : la classe de milieu humide à prédire (`CLASSIF`)\n","- Variables explicatives (X) : l'ensemble des variables numériques prétraitées.\n","\n","**3. Sous-échantillonnage stratifié à 10%**\n","\n","L'algorithme KNN stocke l'ensemble du jeu d'entraînement en mémoire et calcule les distances avec toutes les observations à chaque prédiction. Avec plus d'un million de lignes, cela rendrait l'entraînement et la prédiction extrêmement lents. Nous sous-échantillonnons donc à **10% du dataset** (≈100 000 observations), ce qui reste représentatif tout en étant computationnellement réalisable. Le sous-échantillonnage est stratifié pour conserver la proportion des classes.\n","\n","**4. Division en jeux d'entraînement et de test**\n","\n","Séparation des données sous-échantillonnées en jeu d'entraînement (70%) et de test (30%) avec stratification pour préserver la distribution des classes.\n"],"metadata":{"id":"19dR1dB5zhy6"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"-g5UKcGbdmKO"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","# Définition de X (features) et y (target). On ne sélectionne que les\n","# variables numérique pour X.\n","y_full = df_features_nonan['CLASSIF'].values\n","X_full = df_features_nonan[num_vars].values\n","\n","# Sous-échantillonnage stratifié à 10% du dataset pour rendre\n","# l'entraînement et la prédiction réalisables.\n","X_sample, _, y_sample, _ = train_test_split(\n","    X_full, y_full,\n","    train_size=0.1,\n","    random_state=0,\n","    stratify=y_full\n","    )\n","\n","print(f\"Dataset complet : {X_full.shape[0]:_} observations\".replace(\"_\", \" \"))\n","print(f\"Dataset sous-échantillonné : {X_sample.shape[0]:_} observations ({X_sample.shape[0]/X_full.shape[0]*100:.1f}%)\\n\".replace(\"_\", \" \"))\n","\n","# Division des données sous-échantillonnées en jeu d'entraînement (70%) et\n","# de test (30%)\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X_sample, y_sample,\n","    test_size=0.30,  # 30% pour le test\n","    random_state=0,\n","    stratify=y_sample\n","    )\n","\n","print(f\"Jeu d'entraînement : {X_train.shape[0]:_} observations × {X_train.shape[1]} variables\".replace(\"_\", \" \"))\n","print(f\"Jeu de test : {X_test.shape[0]:_} observations × {X_test.shape[1]} variables\".replace(\"_\", \" \"))"]},{"cell_type":"markdown","source":["### 6.2 Prédiction et optimisation du nombre de voisins\n","\n","Le nombre de voisins **k** est l'hyperparamètre clé de KNN : une valeur trop faible rend le modèle sensible au bruit (surapprentissage), tandis qu'une valeur trop élevée sur-lisse les prédictions (sous-apprentissage).\n","\n","**Normalisation des données**  \n","KNN repose sur le calcul de distances euclidiennes. Sans normalisation, les variables avec de grandes plages de valeurs domineraient le calcul. Nous utilisons `StandardScaler` pour ramener toutes les variables à la même échelle (moyenne = 0, écart-type = 1), garantissant ainsi que chaque variable contribue équitablement à la mesure de similarité.\n","\n","**Recherche du k optimal**  \n","Nous testons différentes valeurs de k (de 3 à 30) et sélectionnons celle qui maximise l'exactitude sur le jeu de test. Le graphique ci-dessous montre que **k=5** donne le meilleur score (78.03%). On observe également qu'au-delà de k=9 voisins, il n'y a aucun gain de performance, les scores décroissant progressivement jusqu'à 77.07% pour k=30.\n"],"metadata":{"id":"0kuVI3NEuImh"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZFNyGuHE0Czu"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.preprocessing import StandardScaler\n","\n","# Normalisation des données.\n","ss = StandardScaler()\n","ss.fit(X_train)\n","\n","# Optimisation du nombre de voisins (n).\n","n_neighbors = [3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 20, 25, 30]\n","scores = np.full(len(n_neighbors), np.nan)\n","\n","for i, n in enumerate(n_neighbors):\n","    print(f\"Prédictions avec {n} voisins... \", end='')\n","\n","    knn = KNeighborsClassifier(n_neighbors=n)\n","    knn.fit(ss.transform(X_train), y_train)\n","\n","    y_pred = knn.predict(ss.transform(X_test))\n","\n","    scores[i] = accuracy_score(y_test, y_pred, normalize=True)\n","\n","    print(f'score={scores[i]*100:0.2f}%')\n","\n","print()\n","\n","fig, ax = plt.subplots()\n","line2d, = ax.plot(n_neighbors, scores * 100, marker='.')\n","ax.set_xticks(np.arange(2, 32, 2))\n","ax.set_xticks(np.arange(30), minor=True)\n","ax.xaxis.grid(True, linestyle='--', alpha=0.7)\n","ax.set_axisbelow(True)\n","ax.axis(xmin=2, xmax=31)\n","\n","print()\n"]},{"cell_type":"markdown","source":["## 7. Prédiction avec un modèle SVC (Support Vector Classifier)\n","\n","Le _Support Vector Classifier (SVC)_ est un algorithme qui cherche à séparer les classes en traçant une frontière qui laisse le plus d'espace possible entre les groupes. L'objectif est de créer une séparation claire et robuste, moins sensible aux variations dans les données. Cette méthode est particulièrement efficace en haute dimension et robuste au surapprentissage.\n","\n","Comme KNN, SVC nécessite des données numériques (il calcule des distances dans un espace vectoriel) qui nécessite une normalisation préalable, car la méthode est très sensible à l'échelle des variables. Nous réutiliserons donc simplement le dataset que nous avons préparé pour le KNN à l'étape 6.1 précédente.\n","\n","SVC peut être plus lent à entraîner que Random Forest ou KNN sur de grands datasets. Nous utiliserons donc un sous-échantillon à 1%."],"metadata":{"id":"vJ8uZ67xCGEk"}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","from sklearn.svm import SVC\n","\n","# Préparation du dataset.\n","y_full = df_features_nonan['CLASSIF'].values\n","X_full = df_features_nonan[num_vars].values\n","\n","X_sample, _, y_sample, _ = train_test_split(\n","    X_full, y_full,\n","    train_size=0.01,\n","    random_state=0,\n","    stratify=y_full\n","    )\n","\n","print(f\"Dataset complet : {X_full.shape[0]:_} observations\".replace(\"_\", \" \"))\n","print(f\"Dataset sous-échantillonné : {X_sample.shape[0]:_} observations ({X_sample.shape[0]/X_full.shape[0]*100:.1f}%)\\n\".replace(\"_\", \" \"))\n","\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X_sample, y_sample,\n","    test_size=0.30,\n","    random_state=0,\n","    stratify=y_sample\n","    )\n","\n","print(f\"Jeu d'entraînement : {X_train.shape[0]:_} observations × {X_train.shape[1]} variables\".replace(\"_\", \" \"))\n","print(f\"Jeu de test : {X_test.shape[0]:_} observations × {X_test.shape[1]} variables\".replace(\"_\", \" \"))\n","\n","# Entraînement et validation du modèle.\n","print(\"\\nEntraînement du modèle SVC en cours...\", end='')\n","svc = SVC(kernel='linear', random_state=0)\n","svc.fit(ss.transform(X_train), y_train)\n","print(' OK')\n","\n","y_pred = svc.predict(ss.transform(X_test))\n","\n","score = accuracy_score(y_test, y_pred, normalize=True)\n","\n","print(f\"\\nExactitude SVC : {score * 100:.2f}%\")"],"metadata":{"id":"RVYezDmVDI-j"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DpWC1kUA3MUY"},"source":["## 8. Prédiction avec un modèle Random Forest\n","\n","Le **Random Forest** est un algorithme d'ensemble qui combine les prédictions de multiples arbres de décision entraînés sur différents sous-échantillons des données. Chaque arbre vote pour une classe, et la classe majoritaire est retenue comme prédiction finale, réduisant ainsi le risque de surapprentissage.\n","\n","Le workflow reste similaire à celui de KNN (préparation, optimisation, évaluation), mais Random Forest présente deux avantages majeurs : il peut traiter **l'ensemble complet du dataset** (> 1 million d'observations) et il accepte les **variables catégorielles**."]},{"cell_type":"markdown","source":["### 8.1 Préparation des données pour le modèle Random Forest\n","\n","Pour des raisons pédagogiques et afin de maintenir des temps de calcul raisonnables, nous sous-échantillonnons à **10% du dataset** comme nous l'avons fait précédemment avec le modèle KNN. Contrairement à KNN toutefois, nous utilisons maintenant **toutes les variables** : numériques et catégorielles.\n","\n","Nous normalisons les variables numériques avec `StandardScaler`. Bien que ce ne soit pas strictement nécessaire pour Random Forest (basé sur des arbres de décision), la littérature suggère que cela peut améliorer légèrement la stabilité et les performances du modèle."],"metadata":{"id":"nmpYrh4d2XGj"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"egz4Zy7DJD0u"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","\n","# Définition de X (features) et y (target). Toutes les variables sont\n","# gardées pour X.\n","y_full = df_features_nonan['CLASSIF'].values\n","X_full = df_features_nonan[num_vars + cat_vars].values\n","\n","# Sous-échantillonnage stratifié à 10% du dataset pour rendre\n","# l'entraînement et la prédiction plus rapide dans le cadre du cours.\n","X_sample, _, y_sample, _ = train_test_split(\n","    X_full, y_full,\n","    train_size=0.1,\n","    random_state=0,\n","    stratify=y_full\n","    )\n","\n","print(f\"Dataset complet : {X_full.shape[0]:_} observations\".replace(\"_\", \" \"))\n","print(f\"Dataset sous-échantillonné : {X_sample.shape[0]:_} observations ({X_sample.shape[0]/X_full.shape[0]*100:.1f}%)\\n\".replace(\"_\", \" \"))\n","\n","# Division des données sous-échantillonnées en jeu d'entraînement (70%) et\n","# de test (30%)\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X_sample, y_sample,\n","    test_size=0.30,  # 30% pour le test\n","    random_state=0,\n","    stratify=y_sample\n","    )\n","\n","print(f\"Jeu d'entraînement : {X_train.shape[0]:_} observations × {X_train.shape[1]} variables\".replace(\"_\", \" \"))\n","print(f\"Jeu de test : {X_test.shape[0]:_} observations × {X_test.shape[1]} variables\".replace(\"_\", \" \"))\n","\n","# Normalisation des variables.\n","scaler = ColumnTransformer(\n","    transformers=[\n","        ('num_imputer', StandardScaler(), num_vars),\n","        ],\n","    remainder='passthrough'\n","    )\n","\n","X_train_std = scaler.fit_transform(\n","    pd.DataFrame(X_train, columns=num_vars + cat_vars)\n","    )\n","X_test_std = scaler.transform(\n","    pd.DataFrame(X_test, columns=num_vars + cat_vars)\n","    )\n"]},{"cell_type":"markdown","source":["### 8.2. Prédiction et optimisation du nombre d'estimateurs\n","\n","Le paramètre **n_estimators** contrôle le nombre d'arbres dans la forêt : plus il est élevé, plus le modèle est stable, mais plus le temps de calcul est long. Nous testons différentes valeurs pour identifier le meilleur compromis.\n","\n","Random Forest possède plusieurs autres hyperparamètres importants (`max_depth`, `min_samples_split`, `max_features`). Des outils comme **GridSearchCV** et **RandomizedSearchCV** permettent d'optimiser plusieurs hyperparamètres simultanément. Ces techniques seront couvertes dans le cours suivant."],"metadata":{"id":"8D0wvRWw56QV"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"5R9hd6UTMZPW"},"outputs":[],"source":["from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import accuracy_score\n","\n","n_estimators = [5, 25, 50, 75]\n","scores = np.full(len(n_estimators), np.nan)\n","\n","for i, n in enumerate(n_estimators):\n","    print(f\"Prédictions avec {n} estimateurs... \", end='')\n","\n","    clf = RandomForestClassifier(n_estimators=n)\n","    clf.fit(X_train_std, y_train)\n","\n","    y_pred = clf.predict(X_test_std)\n","\n","    scores[i] = accuracy_score(y_test, y_pred, normalize = True)\n","\n","    print(f'score={scores[i]*100:0.2f}%')\n","\n","print()\n","\n","fig, ax = plt.subplots()\n","line2d, = ax.plot(n_estimators, scores * 100, marker='.')\n","ax.set_xticks(np.arange(0, 350, 5))\n","ax.set_xticks(np.arange(0, 350, 10))\n","ax.xaxis.grid(True, linestyle='--', alpha=0.7)\n","ax.set_axisbelow(True)\n","ax.axis(ymin=70, ymax=100, xmin=0, xmax=80)\n","\n","print()"]},{"cell_type":"markdown","source":["### 8.3. Importance des variables\n","\n","Un avantage majeur de Random Forest est sa capacité à quantifier l'**importance des variables** (feature importance). Cette métrique indique la contribution de chaque variable aux prédictions du modèle, calculée en mesurant la réduction moyenne de l'impureté (Gini) apportée par chaque variable à travers tous les arbres de la forêt.\n","\n","L'analyse de l'importance des variables permet d'identifier les variables les plus discriminantes pour la classification, de simplifier le modèle en éliminant les variables peu informatives, et d'améliorer l'interprétabilité des résultats. Cette information est particulièrement utile pour comprendre quels facteurs (topographie, hydrologie, etc.) sont les plus déterminants pour caractériser les différentes classes de milieux humides."],"metadata":{"id":"1bJkLF1H-jzC"}},{"cell_type":"code","source":["# Extraction de l'importance des variables.\n","importances = clf.feature_importances_\n","feature_names = num_vars + cat_vars\n","\n","importance_df = pd.DataFrame({\n","    'Variable': feature_names,\n","    'Importance': importances\n","    }).sort_values('Importance', ascending=False)\n","\n","# Visualisation graphique\n","fig, ax = plt.subplots(figsize=(10, 8))\n","top_n = 20  # Nombre de variables à afficher\n","\n","importance_df_top = importance_df.head(top_n)\n","ax.barh(range(top_n), importance_df_top['Importance'], color='skyblue')\n","ax.set_yticks(range(top_n))\n","ax.set_yticklabels(importance_df_top['Variable'])\n","ax.invert_yaxis()  # Variable la plus importante en haut\n","ax.set_xlabel('Importance', fontsize=12)\n","ax.set_title(f'Top {top_n} des variables les plus importantes', fontsize=14, pad=15)\n","ax.grid(axis='x', linestyle='--', alpha=0.7)\n","ax.set_axisbelow(True)\n","\n","fig.tight_layout()\n","plt.show()"],"metadata":{"id":"9brlb-7Z-i8V"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 8.4 Matrice de confusion\n","\n","La **matrice de confusion** visualise les performances du modèle classe par classe. Chaque ligne représente les observations d'une classe réelle, et chaque colonne les prédictions du modèle. Les valeurs sur la **diagonale** (en foncé) représentent les prédictions correctes, tandis que les valeurs hors diagonale indiquent les erreurs de classification.\n","\n","Cette visualisation est particulièrement utile pour identifier :\n","- Les classes bien prédites (valeurs élevées sur la diagonale)\n","- Les confusions fréquentes entre certaines classes (valeurs élevées hors diagonale)\n","- Les biais liés au déséquilibre des classes\n","\n","La matrice normalisée (en pourcentages) permet de comparer les performances entre classes indépendamment de leur nombre d'observations."],"metadata":{"id":"jXSr_X7zITjS"}},{"cell_type":"code","source":["from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n","\n","# Création des deux matrices.\n","cm_counts = confusion_matrix(y_test, y_pred)\n","cm_normalized = confusion_matrix(y_test, y_pred, normalize='true')\n","\n","# Visualisation côte à côte.\n","fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n","\n","# Matrice avec les comptes.\n","disp1 = ConfusionMatrixDisplay(\n","    confusion_matrix=cm_counts, display_labels=clf.classes_)\n","disp1.plot(ax=axes[0], cmap='Blues', values_format='d', colorbar=False)\n","\n","axes[0].set_title('Nombre de prédictions', fontsize=14, pad=15)\n","\n","# Matrice normalisée.\n","disp2 = ConfusionMatrixDisplay(\n","    confusion_matrix=cm_normalized, display_labels=clf.classes_)\n","disp2.plot(ax=axes[1], cmap='Greens', values_format='.2%', colorbar=False)\n","\n","axes[1].set_title('Pourcentages (par classe réelle)', fontsize=12, pad=15)\n","\n","fig.suptitle('Matrices de confusion - Random Forest', fontsize=14, y=1.02)\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"xzss0tFIIhfp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 8.5 Interprétabilité avec les valeurs SHAP\n","\n","Les **valeurs de Shapley (SHAP)** quantifient la contribution de chaque variable à la prédiction d'une classe spécifique. Contrairement à l'importance globale des variables (section 8.3), SHAP explique comment et dans quelle direction chaque variable influence les prédictions.\n","\n","Puisque cette opération est très longue, on va sous-échantillonné à 1% et\n","on ne gardera que les 6 variables les plus importantes tel que déterminé\n","à la section 8.3."],"metadata":{"id":"aJatl_DZQKs4"}},{"cell_type":"code","source":["import shap\n","\n","SELECTED_CLASS = 'OW'\n","# Classes disponibles: 'BOG', 'FEN', 'MAR', 'OW', 'SAV', 'UPL'\n","\n","# Préparation du dataset.\n","y_full = df_features_nonan['CLASSIF'].values\n","X_full = df_features_nonan[num_vars + cat_vars].values\n","\n","X_sample, _, y_sample, _ = train_test_split(\n","    X_full, y_full,\n","    train_size=0.01,\n","    random_state=0,\n","    stratify=y_full\n","    )\n","\n","print(f\"Dataset complet : {X_full.shape[0]:_} observations\".replace(\"_\", \" \"))\n","print(f\"Dataset sous-échantillonné : {X_sample.shape[0]:_} observations ({X_sample.shape[0]/X_full.shape[0]*100:.1f}%)\\n\".replace(\"_\", \" \"))\n","\n","# Normalisation des variables.\n","scaler = ColumnTransformer(\n","    transformers=[\n","        ('num_imputer', StandardScaler(), num_vars),\n","        ],\n","    remainder='passthrough'\n","    )\n","\n","X_sample_std = scaler.fit_transform(\n","    pd.DataFrame(X_sample, columns=num_vars + cat_vars)\n","    )\n","X_sample_std = pd.DataFrame(X_sample_std, columns=num_vars + cat_vars)\n","\n","# Sélection des 10 variables les plus importantes.\n","X_sample_std = X_sample_std.loc[:, importance_df.Variable[:6]]\n","\n","\n","# Création de l'explainer et calcul des valeurs SHAP.\n","print(\"Calcul des valeurs SHAP en cours...\")\n","\n","clf = RandomForestClassifier(n_estimators=25)\n","clf.fit(X_sample_std, y_sample)\n","\n","explainer = shap.TreeExplainer(clf)\n","shap_values = explainer(X_sample_std)\n","\n","# Extraction des valeurs SHAP pour la classe sélectionnée.\n","class_idx = list(clf.classes_).index(SELECTED_CLASS)\n","shap_values_class = shap_values[:, :, class_idx]\n","\n","print()\n","\n","shap.plots.scatter(\n","    shap_values_class,\n","    ylabel=f\"Valeurs SHAP pour la classe '{SELECTED_CLASS}'\"\n","    )\n","plt.title(f\"Valeurs SHAP pour la classe '{SELECTED_CLASS}'\",\n","          fontsize=14, pad=20)\n","\n","fig = plt.gcf()\n","fig.tight_layout()\n","\n","print('\\n\\n')\n","\n","shap.plots.beeswarm(shap_values_class,\n","                    max_display=15,\n","                    show=False)\n","plt.title(f\"Valeurs SHAP pour la classe '{SELECTED_CLASS}'\",\n","          fontsize=14, pad=20)\n","\n","fig = plt.gcf()\n","fig.tight_layout()\n","plt.show()"],"metadata":{"id":"_cobZY3nQLBQ"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}