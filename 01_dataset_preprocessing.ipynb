{"cells":[{"cell_type":"markdown","metadata":{"executionInfo":{"elapsed":85,"status":"ok","timestamp":1769023118111,"user":{"displayName":"Martin Blouin","userId":"06053736196572450606"},"user_tz":300},"id":"ND9e5No84eal","outputId":"88aa9296-3540-4bc2-c0d7-a9e39e5f17b0"},"source":["# Cours #1 - Introduction au Machine Learning pour la Classification des Milieux Humides du Lac Ontario\n","\n","Cours basé sur le chapitre 4 du livre *Python Machine Learning* de S. Raschka  \n","\n","[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/geo-stack/cours_ml_envirocan/blob/main/01_dataset_preprocessing.ipynb)"]},{"cell_type":"markdown","metadata":{"id":"DmOSbOQrB-7x"},"source":["## 1. Configuration de l'environnement"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"omKXhUeaB-7y"},"outputs":[],"source":["print(\"Installation de 'gdown' pour le téléchargement du dataset...\", end='')\n","!pip install --upgrade gdown -q\n","print(' OK')\n","\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import os\n","import gdown\n","\n","print('Environnement configuré avec succès!')"]},{"cell_type":"markdown","metadata":{"id":"Auoju13oB-70"},"source":["## 2. Téléchargement et chargement du jeu de données\n","\n","> ⚠️ _À noter que l’accès au téléchargement du fichier de données est limité à la période du cours. Passé cette période, le fichier ne sera plus accessible._"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cXbWPNORB-70"},"outputs":[],"source":["INPUT_FILENAME = 'CWRM_GLFEI_vegetation_dataset_v2_extracted_landscape.csv'\n","\n","print(\"Téléchargement du jeu de données en cours...\")\n","\n","gdown.download(\n","    f'https://drive.google.com/uc?id=11oKUaykdZ0G5hpns6nct_lkVvIzo_3w3',\n","    INPUT_FILENAME,\n","    quiet=True\n","    )\n","\n","print(\"Téléchargement terminé.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0ikEC9YA34BD"},"outputs":[],"source":["dtype_spec = {\n","    'Land_mask': 'boolean',\n","    'DOMINANCE': 'string',\n","    }\n","\n","df = pd.read_csv(INPUT_FILENAME, delimiter=\";\", dtype=dtype_spec)\n","df.head()"]},{"cell_type":"markdown","source":["## 3. Exploration du jeu de données\n","\n","Le jeu de données contient des observations de milieux humides du Lac Ontario qui sont directement connectés et influencés par le Lac. Les données proviennent de différentes sources, pour lesquels des variables physiques représentant l'hydro-période, hydrologie, la topographie et le paysage ont été calculées (profondeur, % inondé, période de fluctuation, type de sol, type hydrogéomorphique, pente, etc.). Cette base de données est utilisée pour calibrer un classificateur prédisant une classe de milieu humide à un point donné qui est intégré dans le Coastal Wetland Response Model (CWRM).\n","\n","<details>\n","<summary>Tableau 1 : Description des classes de milieu humide à prédire</summary>\n","\n","<br>\n","\n","| Acronyme | Description                          |\n","|----------|--------------------------------------|\n","| OW       | Open Water                          |\n","| SAV      | Submerged Aquatic Vegetation        |\n","| EM       | Emergent marshes                    |\n","| WM       | Wet Meadow                          |\n","| SW       | Swamp                               |\n","| UPL      | Upland                              |\n","\n","</details>\n","\n","<br>\n","\n","<details>\n","<summary>Tableau 2 : Nom et description des colonnes du jeu de données</summary>\n","  \n","<br>\n","\n","| N° | Colonne                 | Description                                                                                 |\n","|----|-------------------------|---------------------------------------------------------------------------------------------|\n","|  1 | SAMPLE_ID               | Identifiant unique de l'échantillon                                                        |\n","|  2 | PT_ID                   | Identifiant unique du point de grille                                                      |\n","|  3 | SITE_ID                 | Identiifiant unique du site de milieux humide                                              |\n","|  4 | TILE_ID                 | Identifiant unique de la tuile du DEM                                                      |\n","|  5 | LON                     | Coordonnée Longitude (degrés décimaux)                                                     |\n","|  6 | LAT                     | Coordonnée Latitude (degrés décimaux)                                                      |\n","|  7 | XVAL                    | Coordonnée Y en UTM (m) (EPSG :32617 ou 32618)                                             |\n","|  8 | YVAL                    | Coordonnée Y en UTM (m) (EPSG :32617 ou 32618)                                             |\n","|  9 | ZVAL_1m                 | Coordonnée Z à partir du DEM 1m en IGLD (m) pour quadrats seulement                        |\n","| 10 | ZVAL                    | Coordonnée Z à partir du DEM 10m en IGLD (m)                                               |\n","| 11 | YEAR                    | Année d’échantillonnage                                                                    |\n","| 12 | SOURCE                  | Jeu de données source du relevé                                                            |\n","| 13 | SOURCE_TYPE             | 'Field Survey' ou 'Photo Interpretation'                                                   |\n","| 14 | SAV                     | % recouvrement total des espèces de SAV (quadrats seulement)                               |\n","| 15 | EM                      | % recouvrement total des espèces de EM (quadrats seulement)                                |\n","| 16 | WM                      | % recouvrement total des espèces de WM (quadrats seulement)                                |\n","| 17 | SW                      | % recouvrement total des espèces de SW (quadrats seulement)                                |\n","| 18 | UPL                     | % recouvrement total des espèces de UPL (quadrats seulement)                               |\n","| 19 | DOMINANCE               | Permet de savoir si le quadrat était mixé ou dominant lors de l’assignation de la classe   |\n","| 20 | CLASSIF                 | Classe assignée                                                                            |\n","| 21 | UTM                     | Zone UTM (17 ou 18)                                                                        |\n","| 22 | SLOPE_10m               | Pente sur résolution spatiale de 10m                                                       |\n","| 23 | CURVATURE_10m           | Pente seconde sur résolution spatiale de 10m                                               |\n","| 24 | SLOPE_30m_resampled     | Pente sur résolution spatiale de 30m                                                       |\n","| 25 | CURVATURE_30m_resampled | Pente seconde sur résolution spatiale de 30m                                               |\n","| 26 | SLOPE_30m_resampled     | Pente sur résolution spatiale de 30m                                                       |\n","| 27 | CURVATURE_30m_resampled | Pente seconde sur résolution spatiale de 30m                                               |\n","| 28 | SLOPE_130m_resampled    | Pente sur résolution spatiale de 130m                                                      |\n","| 29 | CURVATURE_130m_resampled| Pente seconde sur résolution spatiale de 130m                                              |\n","| 30 | SLOPE_250m_resampled    | Pente sur résolution spatiale de 250m                                                      |\n","| 31 | CURVATURE_250m_resampled| Pente seconde sur résolution spatiale de 250m                                              |\n","| 32 | SLOPE_310m_resampled    | Pente sur résolution spatiale de 310m                                                      |\n","| 33 | CURVATURE_310m_resampled| Pente seconde sur résolution spatiale de 310m                                              |\n","| 34 | hMin_SC1                | Profondeur minimale; m.   (année précédente)                                               |\n","| 35 | hMoy_SC1                | Profondeur moyenne; m.   (année précédente)                                                |\n","| 36 | hMax_SC1                | Profondeur maximale; m.   (année précédente)                                               |\n","| 37 | Flood_SC1               | % temps inondé (année précédente)                                                          |\n","| 38 | PcFMoy_SC1              | % temps strictement inondé; sans fluctuations (année précédente)                          |\n","| 39 | pcDMoy_SC1              | % temps strictement exondé; sans fluctuations (année précédente)                          |\n","| 40 | pcFluxMoy_SC1           | % temps fluctuations (année précédente)                                                    |\n","| 41 | saPxMax_SC1             | Puissance maximale normalisée de l'analyse en ondelettes (année précédente)                |\n","| 42 | saPMoy_SC1              | Puissance moyenne normalisée de l'analyse en ondelettes (année précédente)                 |\n","| 43 | TcMin_SC1               | Période minimale des fluctuations (année précédente)                                       |\n","| 44 | TcMax_SC1               | Période maximale des fluctuations (année précédente)                                       |\n","| 45 | TcMoy_SC1               | Période moyenne des fluctuations (année précédente)                                        |\n","| 46 | xTcMax_SC1              | Période maximale détectable des fluctuations (année précédente)                            |\n","| 47 | hMin_PSC2               | Profondeur minimale; m. (2 ans précédents)                                                |\n","| 48 | hMoy_PSC2               | Profondeur moyenne; m.  (2 ans précédents)                                                |\n","| 49 | hMax_PSC2               | Profondeur maximale; m.  (2 ans précédents)                                               |\n","| 50 | Flood_PSC2              | % temps inondé (2 ans précédents)                                                         |\n","| 51 | PcFMoy_PSC2             | % temps strictement inondé; sans fluctuations (2 ans précédents)                          |\n","| 52 | pcDMoy_PSC2             | % temps strictement exondé; sans fluctuations (2 ans précédents)                          |\n","| 53 | pcFluxMoy_ PSC2         | % temps fluctuations (2 ans précédents)                                                   |\n","| 54 | saPxMax_ PSC2           | Puissance maximale normalisée de l'analyse en ondelettes (2 ans précédents)               |\n","| 55 | saPMoy_ PSC2            | Puissance moyenne normalisée de l'analyse en ondelettes (2 ans précédents)                |\n","| 56 | TcMin_ PSC2             | Période minimale des fluctuations (2 ans précédents)                                      |\n","| 57 | TcMax_ PSC2             | Période maximale des fluctuations (2 ans précédents)                                      |\n","| 58 | TcMoy_ PSC2             | Période moyenne des fluctuations (2 ans précédents)                                       |\n","| 59 | xTcMax_ PSC2            | Période maximale détectable des fluctuations (2 ans précédents)                           |\n","| 60 | hMin_PSC3               | Profondeur minimale; m. (3 ans précédents)                                                |\n","| 61 | hMoy_PSC3               | Profondeur moyenne; m.  (3 ans précédents)                                                |\n","| 62 | hMax_PSC3               | Profondeur maximale; m.  (3 ans précédents)                                               |\n","| 63 | Flood_PSC3              | % temps inondé (3 ans précédents)                                                         |\n","| 64 | PcFMoy_PSC3             | % temps strictement inondé; sans fluctuations (3 ans précédents)                          |\n","| 65 | pcDMoy_PSC3             | % temps strictement exondé; sans fluctuations (3 ans précédents)                          |\n","| 66 | pcFluxMoy_ PSC3         | % temps fluctuations (3 ans précédents)                                                   |\n","| 67 | saPxMax_ PSC3           | Puissance maximale normalisée de l'analyse en ondelettes (3 ans précédents)               |\n","| 68 | saPMoy_ PSC3            | Puissance moyenne normalisée de l'analyse en ondelettes (3 ans précédents)                |\n","| 69 | TcMin_ PSC3             | Période minimale des fluctuations (3 ans précédents)                                      |\n","| 70 | TcMax_ PSC3             | Période maximale des fluctuations (3 ans précédents)                                      |\n","| 71 | TcMoy_ PSC3             | Période moyenne des fluctuations (3 ans précédents)                                       |\n","| 72 | xTcMax_ PSC3            | Période maximale détectable des fluctuations (3 ans précédents)                           |\n","| 73 | Organic                 | Type de sol organique (binaire)                                                            |\n","| 74 | Mineral                 | Type de sol minéral (binaire)                                                              |\n","| 75 | Rock                    | Type de sol rocheux (binaire)                                                              |\n","| 76 | Calcareous              | Type de sol calcaire (binaire)                                                             |\n","| 77 | Non-Calcar              | Type de sol non-calcaire (binaire)                                                         |\n","| 78 | Coarse                  | Type de substrat grossier (binaire)                                                        |\n","| 79 | Fine                    | Type de substrat fin (binaire)                                                             |\n","| 80 | Class_Land              | Classe d'occupation du sol                                                                 |\n","| 81 | Land_mask               | Si masqué ou non selon la classe d'occupation du sol (boolean)                             |\n","| 82 | HGM_BL                  | Type hydrogéomorphique : Barrier Lagoon (binaire)                                          |\n","| 83 | HGM_LOE                 | Type hydrogéomorphique : Baie Ouverte Lacustre (binaire)                                   |\n","| 84 | HGM_LPP                 | Type hydrogéomorphique : Baie Protégée Lacustre (binaire)                                  |\n","| 85 | HGM_RRB                 | Type hydrogéomorphique : Embouchure de rivière Barrée (binaire)                            |\n","| 86 | HGM_RRO                 | Type hydrogéomorphique : Embouchure de rivière Ouverte (binaire)                           |\n","\n","</details>\n","<br>"],"metadata":{"id":"gESTaZrWUKmF"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZnyMbAts6uTn"},"outputs":[],"source":["# Génération d'un histogramme du nombre d'observations par classe ('CLASSIF').\n","\n","import matplotlib.ticker as ticker\n","\n","counts = df.CLASSIF.value_counts()\n","\n","fig, ax = plt.subplots()\n","bars = ax.bar(counts.index, counts.values, color='skyblue')\n","ax.set_xlabel('Classes', fontsize=14, labelpad=10)\n","ax.set_ylabel('Nombre', fontsize=14, labelpad=10)\n","ax.set_title(\"Histogramme des classes de milieu humide\")\n","ax.yaxis.grid(True, linestyle='--', alpha=0.7)\n","ax.set_axisbelow(True)\n","ax.yaxis.set_major_formatter(\n","    ticker.FuncFormatter(\n","        lambda x, _: f'{x/1000:0.0f}k' if x >= 1000 else int(x)\n","        ))\n","\n","# Ajout padding vertical (10 % au-dessus du max).\n","ax.set_ylim(top=counts.max() * 1.1)\n","\n","# Ajout de la valeur au dessus de chaque barre.\n","for bar in bars:\n","    count = bar.get_height()\n","    ax.text(bar.get_x() + bar.get_width() / 2, count + 0.1,\n","            f\"{count:,}\".replace(\",\", \" \"),\n","            ha='center', va='bottom', fontsize=10)\n","\n","fig.tight_layout()"]},{"cell_type":"markdown","source":["## 4. Gestion des données manquantes"],"metadata":{"id":"vkoKyMadUu7e"}},{"cell_type":"markdown","source":["### 4.1. Quantification et visualisation des données manquantes"],"metadata":{"id":"MBDjnt6XgBI7"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"fGuY4zEs643W"},"outputs":[],"source":["# Génération d'un histogramme\n","\n","# Quantification des données manquantes\n","nan_perc = df.isna().mean()\n","\n","# N'inclure que les variables avec au moins une valeur manquante.\n","nan_perc_filt = nan_perc[nan_perc > 0].sort_values(ascending=False)\n","\n","# Afficher les résultats dans un graphique.\n","fig, ax = plt.subplots(figsize=(10, 6))\n","bars = ax.bar(\n","    nan_perc_filt.index,\n","    nan_perc_filt.values * 100,\n","    color='salmon'\n","    )\n","ax.set_title('Variables avec valeurs manquantes (NaN > 0)')\n","ax.set_ylabel('Proportion de NaNs [%]', fontsize=14, labelpad=10)\n","ax.yaxis.grid(True, linestyle='--', alpha=0.7)\n","ax.set_axisbelow(True)\n","ax.set_yticks(np.arange(0, 110, 10))\n","for label in ax.get_xticklabels():\n","    label.set_fontsize(8)\n","    label.set_rotation(45)\n","    label.set_ha('right')\n","\n","fig.tight_layout()"]},{"cell_type":"markdown","source":["### 4.2. Suppression des colonnes avec valeurs manquantes excessives\n","\n","On élimine les colonnes avec un % de données manquantes supérieure à 30%."],"metadata":{"id":"cUInjT4bcBeQ"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"BBxYkD6tF9qd"},"outputs":[],"source":["cols_to_drop = nan_perc[nan_perc > 0.3].index.tolist()\n","df_cleaned = df.drop(columns=cols_to_drop)\n","\n","print(\n","    f\"Les {len(cols_to_drop)} colonnes suivantes ont été retirées du jeu \"\n","    f\"de données car elles contiennent plus de 30% de valeurs manquantes :\\n\"\n","    + \"\\n\".join(f\"   - {col} ({nan_perc[col] * 100:0.1f}%)\" for\n","                col in cols_to_drop)\n","    )"]},{"cell_type":"markdown","source":["### 4.3 Sélection des variables explicatives\n","\n","On retire ici les colonnes pouvant biaiser la prédiction (ex: localisation,\n","identifiants) ainsi que la variable à prédire `CLASSIF` pour composer\n","les variables explicatives."],"metadata":{"id":"sGc39Za1dFGM"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y5GNkDOVFThQ"},"outputs":[],"source":["cols_to_drop = [\n","    'SAMPLE_ID', 'PT_ID', 'SITE_ID', 'TILE_ID', 'LON', 'LAT', 'XVAL',\n","    'YVAL', 'ZVAL', 'YEAR', 'SOURCE', 'SOURCE_TYPE', 'CLASSIF', 'UTM'\n","    ]\n","df_features = df_cleaned.drop(columns=cols_to_drop)\n","\n","print(\n","    f\"Les {len(cols_to_drop)} colonnes suivantes ont été retirées du jeu \"\n","    f\"de données car elles pourraient potentiellement biaiser le \"\n","    f\"modèle de prédiction :\\n\"\n","    + \"\\n\".join(f\"   - {col}\" for col in cols_to_drop)\n","    )"]},{"cell_type":"markdown","source":["### 4.3 Imputation des valeurs manquantes et encodage des variables catégorielles\n","\n","Pour traiter les valeurs manquantes restantes dans le jeu de données, on\n","distingue deux approches :\n","\n","1. Supprimer les lignes contenant des valeurs manquantes (perte d'information)\n","\n","2. Imputer les valeurs manquantes, c'est-à-dire les remplacer par une valeur estimée\n","\n","Dans cette la cellule ci-dessous, nous choisissons d'imputer les valeurs manquantes à l'aide de stratégies adaptées au type de variable. Nous utilisons pour cela la classe `ColumnTransformer`, qui permet d'appliquer sélectivement les bons traitements à chaque groupe de variables.\n","\n","On distingue deux types de variables :\n","\n","1. Pour les **variables numériques** : nous utilisons la méthode `SimpleImputer` de scikit-learn avec la stratégie de la moyenne, ce qui permet de remplacer chaque donnée manquante par la moyenne des valeurs observées dans la colonne concernée.\n","\n","2. Pour les **variables catégorielles** : nous utilisons l'encodeur ordinal\n"," (`OrdinalEncoder`) de scikit-learn. Cette transformation attribue un code numérique unique à chaque modalité rencontrée dans une colonne catégorielle.<br>Attention : cette méthode n'est généralement adéquate que si l'ordre attribué n'a pas d'importance, ou pour des algorithmes supervisés au sens large. Les éventuelles valeurs manquantes dans les colonnes catégorielles seront considérées comme une nouvelle catégorie inconnue et codées en conséquence."],"metadata":{"id":"B-JPhCWxbo3L"}},{"cell_type":"code","source":["from sklearn.compose import ColumnTransformer\n","from sklearn.preprocessing import OrdinalEncoder\n","from sklearn.impute import SimpleImputer\n","\n","# On définit les colonnes qui sont des variables catégorielles et celles\n","# qui sont numériques.\n","cat_cols = [\n","    'Calcareous', 'Class_Land', 'Coarse', 'Fine',\n","    'HGM_BL', 'HGM_LOE', 'HGM_LPP', 'HGM_RRB', 'HGM_RRO',\n","    'Land_mask', 'Mineral', 'Non-Calcar', 'Organic', 'Rock'\n","    ]\n","\n","num_cols = list(df_features.columns[~np.isin(df_features.columns, cat_cols)])\n","\n","# Création du ColumnTransformer, où l'on associe des transformateurs\n","# spécifiques à des colonnes spécifiques.\n","preprocessor = ColumnTransformer(\n","    transformers=[\n","        ('num_imputer', SimpleImputer(strategy='mean'), num_cols),\n","        ('cat_imputer', OrdinalEncoder(), cat_cols)\n","        ],\n","    remainder='passthrough'  # 'passthrough' garde les colonnes non mentionnées\n","    )\n","\n","# Application de la transformation pour imputer les valeurs manquantes.\n","# Le résultat est un tableau NumPy prêt à être utilisé pour l'entraînement\n","# d'un modèle..\n","arr_features = preprocessor.fit_transform(df_features)"],"metadata":{"id":"B2GCUbytpSzm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zf2b5jQKdI4n"},"source":["## 5. Prédiction avec un modèle KNN (K Nearest neighbours)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-g5UKcGbdmKO"},"outputs":[],"source":["# The use of categorical variables with KnN model is not recommended, so it is\n","# better to drop them.\n","mask = np.isin(df_features.columns, num_cols)\n","arr_features_num = arr_features[:, mask]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dUPccDWJcFnZ"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","# On cherche à classer la variable 'CLASSIF'\n","y = df['CLASSIF'].values\n","X = imputed_data\n","\n","# Division : 70% train, 30% test\n","# stratify=y assure que la proportion des classes est conservée dans les deux sets\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.3, random_state=0, stratify=y\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZFNyGuHE0Czu"},"outputs":[],"source":["from sklearn.metrics import accuracy_score\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.preprocessing import StandardScaler\n","\n","# Normalisation des données\n","ss = StandardScaler()\n","\n","neighbours = np.arange(3, 30)\n","scores = []\n","for n in neighbours:\n","  knn = KNeighborsClassifier(n_neighbors=n)\n","  knn.fit(ss.fit_transform(X_train), y_train)\n","  y_pred = knn.predict(ss.transform(X_test))\n","  scores.append(accuracy_score(y_test, y_pred, normalize = True))\n","\n","plt.plot(neighbours, scores)"]},{"cell_type":"markdown","metadata":{"id":"DpWC1kUA3MUY"},"source":["## 6. Prédiction avec un modèle Random Forest"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"94ZQNHKHSdFC"},"outputs":[],"source":["\n","scaler = ColumnTransformer(\n","    transformers=[\n","        # (name, transformer, columns_to_apply_to)\n","        ('num_imputer', StandardScaler(), num_cols),\n","    ],\n","    # 'passthrough' keeps columns not mentioned; 'drop' (default) removes them\n","    remainder='passthrough'\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"egz4Zy7DJD0u"},"outputs":[],"source":["# On cherche à classer la variable 'CLASSIF' encore\n","y = df['CLASSIF'].values\n","X = df_imputed_array\n","# Division : 70% train, 30% test\n","# stratify=y assure que la proportion des classes est conservée dans les deux sets\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.3, random_state=0, stratify=y\n",")\n","\n","X_train_std = scaler.fit_transform(pd.DataFrame(X_train, columns=num_cols + cat_cols))\n","X_test_std = scaler.transform(pd.DataFrame(X_test, columns=num_cols + cat_cols))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5R9hd6UTMZPW"},"outputs":[],"source":["from sklearn.ensemble import RandomForestClassifier\n","\n","estimators = np.arange(50, 1000, 50)\n","scores = []\n","for n in estimators:\n","  clf = RandomForestClassifier(n_estimators=n)\n","  clf.fit(X_train_std, y_train)\n","  y_pred = clf.predict(X_test_std)\n","  scores.append(accuracy_score(y_test, y_pred, normalize = True))\n","plt.plot(estimators, scores)"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python [conda env:base] *","language":"python","name":"conda-base-py"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.13.5"}},"nbformat":4,"nbformat_minor":0}